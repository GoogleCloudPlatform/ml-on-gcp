{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnkK6UERu5C5"
   },
   "source": [
    "# TensorFlow Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TensorFlow versions:* 1.2.\\*, 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators provide a high-level interface to TensorFlow. They are heavily inspired by the [scikit-learn](http://scikit-learn.org/stable/index.html) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLC4hQfkw9Cb"
   },
   "source": [
    "## What are they good for?\n",
    "\n",
    "1. Wrapping your model up in an Estimator gives you off-the-shelf [between-graph replication](https://www.tensorflow.org/versions/r0.12/how_tos/distributed/#replicated_training) for your training jobs. (This makes it particularly easy to take advantage of the scaling functionality on Cloud Machine Learning Engine, for example.)\n",
    "\n",
    "2. Using the Estimator interface makes it easy to save and restore your trained models, which has generally been something of a stumbling block when it comes to using TensorFlow.\n",
    "\n",
    "3. TensorFlow has very useful utilities for models conforming to the Estimator interface. Primary amongst them is the [Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment), which encapsulates the process of training a model.\n",
    "\n",
    "4. TensorFlow has a number of canned Estimators -- [LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier), [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearRegressor), [DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier), [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor). Moreover, it is recommended that model definitions be wrapped up inside estimators. This means that more and more people are going to be using estimators. Defining your models within Estimators allow them to be dropped in with minimal fuss in place of existing Estimators within particular workflows. This allows not just you, but all potential users of your estimator to iterate painlessly.\n",
    "\n",
    "5. If you are interested in (eventually) training your model on [TPUs](https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu), wrapping it in an Estimator will go very far towards making sure that it is truly leveraging the power of the TPU. This is because TensorFlow uses the interface to determine optimal placement of operations on devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A note on Estimator versions\n",
    "\n",
    "TensorFlow currently contains two different Estimator interfaces:\n",
    "\n",
    "1. [tf.estimator.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n",
    "\n",
    "2. [tf.contrib.learn.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n",
    "\n",
    "`tf.estimator.Estimator` is the current interface, and is the one that we will be using in this notebook. However, we will also be using some utilities that make it easier to work with estimators (like experiments), which have not yet been ported over to the `tf.estimator` module. This will require some rather hacky modifications to certain `tf.contrib.learn` concepts. As TensorFlow improves the situation with `tf.estimator`, we will reflect those changes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4h2qPnwDUcY"
   },
   "source": [
    "## Agenda\n",
    "\n",
    "We will describe the interface and implement a lightweight Estimator to perform [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "In this initial example, we will be storing all our data in memory. However, this is not the typical use case for TensorFlow training. So we will, in a bonus section, extend our example to read data from files. This extension is intended to highlight good practices, and the intention is that you be able to paste the input function we produce into your own code and instantly benefit from its performance advantages over [manually feeding data into your model](https://www.tensorflow.org/programmers_guide/reading_data).\n",
    "\n",
    "We will set up an [Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment) to manage model training and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oNlcq5m0u5C7"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEeqnOiju5DB"
   },
   "source": [
    "## tf.estimator.Estimator\n",
    "\n",
    "[Documentation](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator), [Code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/estimator/estimator.py)\n",
    "\n",
    "This class defines the higher-level TensorFlow estimator interface.\n",
    "\n",
    "If you want to construct your own estimator, you must define a function which constructs the appropriate TensorFlow model (graph + operations). The estimator is then constructed by initializing a `tf.estimator.Estimator` by passing your model function as the `model_fn` argument and, optionally:\n",
    "\n",
    "1. `model_dir` -- Directory in which to save information about the model.\n",
    "\n",
    "2. `config` -- [RunConfig](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig) to be used when running the estimator, describing properties extrinsic to the model like the cluster specification, checkpointing strategy, etc.\n",
    "\n",
    "3. `params` -- Dictionary of model hyperparameters.\n",
    "\n",
    "The bulk of the work is in implementing your `model_fn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A short note on RunConfig\n",
    "\n",
    "There are two different versions of RunConfig:\n",
    "\n",
    "1. [tf.estimator.RunConfig](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig)\n",
    "\n",
    "2. [tf.contrib.learn.RunConfig](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig)\n",
    "\n",
    "We will use `tf.contrib.learn.RunConfig` in this notebook because `tf.estimator.RunConfig` breaks compatibility with [tf.contrib.learn.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment), which is a very useful utility that we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UA64md12u5DD"
   },
   "source": [
    "### model_fn\n",
    "\n",
    "##### Accepts\n",
    "\n",
    "1. `features` -- Input to the model. Either a single tensor or a dictionary of labelled (by the string key) tensors.\n",
    "\n",
    "2. `labels` -- Labels corresponding to the provided features (in [PREDICT](https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys) mode, this is guaranteed to be `None`). This can be a single tensor or, in the case of a [multi-headed model](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/multi_head) (multiple heads at output layer), this can be a dictionary of tensors. (In the multi-headed model case, it is not clear what the keys have to be. Presumably they have to match the head variable names?)\n",
    "\n",
    "3. `mode` -- The [mode](https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys) in which the model is to be run.\n",
    "\n",
    "4. `params` -- This is where the hyperparameters passed to the `Estimator` upon instantiation are used. They are used transparently.\n",
    "\n",
    "5. `config` -- `RunConfig` passed to the `Estimator` upon instantiation is made available to the `model_fn` here.\n",
    "\n",
    "##### Returns\n",
    "\n",
    "An [EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec). This is what allows constructed model to play nicely with the TensorFlow `Estimator` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xt49aVsEu5DF"
   },
   "source": [
    "### Running an estimator\n",
    "\n",
    "An `Estimator` has three methods that qualify as a run: `evaluate`, `predict`, and `train`. Each of these methods accepts as its first argument an input function, `input_fn`.\n",
    "\n",
    "The `input_fn` should be callable with no arguments and should return an ordered pair of `features` and `labels` with each being either a tensor or a string-keyed dictionary of tensors.\n",
    "\n",
    "When you call the `evaluate`, `predict`, or `train` method of an estimator:\n",
    "\n",
    "1. `input_fn` is called to produce `features` and `labels`.\n",
    "\n",
    "2. These are passed to the `model_fn` the `Estimator` was instantiated with.\n",
    "\n",
    "3. The resulting model instance has its variables populated from a checkpoint in the model directory that was specified to the `Estimator` upon instantiation. If no checkpoint is specified, the latest one is used. If no checkpoint exists, the variables are initialized.\n",
    "\n",
    "4. Depending on the mode, the appropriate operations are run in a TensorFlow [MonitoredSession](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession). The results of these operations are yielded back to the caller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1b_n6-vu5DH"
   },
   "source": [
    "## Example: Logistic Regression\n",
    "\n",
    "Let us implement an estimator which performs logistic regression and uses it for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqiD_hbEu5DI"
   },
   "source": [
    "### Data\n",
    "\n",
    "Our classifier will perform a logistic regression on a single feature. Let us say that there are two latent classes, $0$ and $1$, corresponding to this feature.\n",
    "\n",
    "Within class $0$, the values for the feature are distributed as the square of a normally distributed random variable with mean $m$ and variance $\\sigma^2$.\n",
    "\n",
    "Within class $1$, the values for the feature are exponentially distributed with rate $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AnnHlbdu5DK"
   },
   "source": [
    "#### Sampling\n",
    "\n",
    "The following functions allow us to sample from each of these two classes of distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "eZLOjS8Du5DM"
   },
   "outputs": [],
   "source": [
    "def sample_from_normal_distribution(n, mu, sigma):\n",
    "    return np.square(np.random.normal(mu, sigma, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MU = 10\n",
    "SIGMA = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 102,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1500587213584,
     "user": {
      "displayName": "Neeraj Kashyap",
      "photoUrl": "//lh4.googleusercontent.com/-tQnVgNJEhwU/AAAAAAAAAAI/AAAAAAAAAH0/apPVgI_Oc5s/s50-c-k-no/photo.jpg",
      "userId": "116654443916371036868"
     },
     "user_tz": 420
    },
    "id": "nkfwKSv9u5DR",
    "outputId": "0378fbc9-afa8-48be-dfc2-f9a53c3c9ab5"
   },
   "outputs": [],
   "source": [
    "example_0 = sample_from_normal_distribution(20, MU, SIGMA)\n",
    "\n",
    "example_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HVRq9nmGu5Db"
   },
   "outputs": [],
   "source": [
    "def sample_from_exponential_distribution(n, rate):\n",
    "    return np.random.exponential(float(1)/rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1500587220719,
     "user": {
      "displayName": "Neeraj Kashyap",
      "photoUrl": "//lh4.googleusercontent.com/-tQnVgNJEhwU/AAAAAAAAAAI/AAAAAAAAAH0/apPVgI_Oc5s/s50-c-k-no/photo.jpg",
      "userId": "116654443916371036868"
     },
     "user_tz": 420
    },
    "id": "C2alIK4su5De",
    "outputId": "b6681007-3ac3-4385-c522-792af80ce132"
   },
   "outputs": [],
   "source": [
    "example_1 = sample_from_exponential_distribution(20, RATE)\n",
    "\n",
    "example_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilYQsjgAu5Dk"
   },
   "source": [
    "#### Input functions\n",
    "\n",
    "We can use these samplers to make input functions for our estimator.\n",
    "\n",
    "It is generally good practice to wrap up our inputs into a [TensorFlow queue](https://www.tensorflow.org/programmers_guide/threading_and_queues) to produce the appropriate tensors in our graph, so let us do that. The nice thing about this is that we can use the queue itself to shuffle up our data rather than having to do it ourselves.\n",
    "\n",
    "In the `generate_input_fn` below, we produce our input data as a numpy `ndarray`. We then use the [tf.train.input_producer](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/input_producer) as the entry point into the TensorFlow framework.\n",
    "\n",
    "The `BATCH_SIZE` parameter determines how many data points we feed to our estimator at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "CbieZK8ru5Dl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "T4Qw1V0Nu5Dq"
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(distributions, num_samples, num_epochs=None, shuffle=True, batch_size=BATCH_SIZE):\n",
    "    assert len(distributions) == len(num_samples)\n",
    "    components = len(distributions)\n",
    "    \n",
    "    samples = np.expand_dims(np.concatenate([distributions[i](num_samples[i]) for i in range(components)]), 1)\n",
    "    labels = np.concatenate([np.full((num_samples[i], 1), i, dtype=np.float64) for i in range(components)])\n",
    "    stack = np.stack([samples, labels], axis=1)\n",
    "    \n",
    "    def input_fn():\n",
    "        q = tf.train.input_producer(stack, shuffle=shuffle, num_epochs=num_epochs)\n",
    "        top = q.dequeue_up_to(batch_size)\n",
    "        \n",
    "        # Split up the features and the labels (in that order).\n",
    "        raw_features, labels = tuple(tf.unstack(top, axis=1))\n",
    "        \n",
    "        # Wrap the features inside a dictionary\n",
    "        return {'inputs': raw_features}, labels\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7bcPqAdu5Dv"
   },
   "source": [
    "### Estimator\n",
    "\n",
    "Let us turn our attention to the estimator definition.\n",
    "\n",
    "#### Model function\n",
    "\n",
    "As we discussed [above](#tf.estimator.Estimator), everything hinges on our [model function definition](#model_fn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JA98urzZu5Dw"
   },
   "outputs": [],
   "source": [
    "MODES = tf.estimator.ModeKeys\n",
    "\n",
    "def model_fn(features, labels, mode, params=None, config=None):\n",
    "    print(features, labels)\n",
    "    # Default learning rate is 0.1\n",
    "    if params is None:\n",
    "        params = {'learning_rate': 0.1}\n",
    "    \n",
    "    inputs = features.get('inputs')\n",
    "    if inputs is None:\n",
    "        raise ValueError('Input \"features\" did not define a \"feature\" key')\n",
    "\n",
    "    # We logistically estimate the probability of each sample belonging to the class labelled 1\n",
    "    weight = tf.Variable(tf.zeros([1,1], dtype=tf.float64), name='weight')\n",
    "    bias = tf.Variable(tf.zeros([1,1], dtype=tf.float64), name='bias')\n",
    "    logit = tf.add(tf.matmul(inputs, weight), bias, name='logit')\n",
    "    logistic = tf.sigmoid(logit, name='logistic')\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    if mode in (MODES.TRAIN, MODES.EVAL):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=labels, name='loss')\n",
    "        )\n",
    "        tf.summary.scalar('loss', loss)        \n",
    "\n",
    "    if mode == MODES.TRAIN:\n",
    "        learning_rate = params.get('learning_rate')\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(\n",
    "            loss, global_step=global_step)\n",
    "\n",
    "    prediction_output = tf.estimator.export.PredictOutput({'class_1_probability': logistic})\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=logistic,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        export_outputs={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_output}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhi0aNLPu5D0"
   },
   "source": [
    "#### Creating an estimator from the model function\n",
    "\n",
    "With the `model_fn` defined, we can now instantiate an estimator.\n",
    "\n",
    "We will store details about the model in `MODEL_DIR`. You can change the location by updating the variable below. The cool thing about it is that `MODEL_DIR` can even be a [Google Cloud Storage bucket](https://cloud.google.com/storage/docs/).\n",
    "\n",
    "We will use the `LEARNING_RATE` variable to specify a value for the learning rate of the trainer used by our estimator. Please edit this as you please."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setup_directory\n",
    "\n",
    "In this tutorial, you will have to specify a few different directories for use with various estimators and (later) to store some data. You should specify fresh directories. To make sure that everything is set up as intended, the function below will accept a directory path from you and:\n",
    "\n",
    "1. Ensure that a fresh directory is created along that path.\n",
    "\n",
    "2. Raise errors if there is already something at that path.\n",
    "\n",
    "3. Act as a marker for the cells in which you should add directories. Simply do a `CTRL+F` for \"setup_directory\" and (excluding this cell and the one below it) you will find the cells that you should modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup_directory(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "NVNCbAEI8C9g"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = setup_directory('DIRECTORY PATH GOES HERE - IT SHOULD NOT POINT TO AN EXISTING DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wmm2Pxqeu5D1"
   },
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig().replace(save_summary_steps=10)\n",
    "regressor = tf.estimator.Estimator(model_fn, model_dir=MODEL_DIR, params={'learning_rate': LEARNING_RATE},\n",
    "                                   config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuse\n",
    "\n",
    "Before we move onto training, evaluation, and prediction, it is worth pausing to discuss how the estimator allows for model reuse (for example, making use of previous training). Generally, in TensorFlow, there are two different means of storing and restoring the state of a TensorFlow graph:\n",
    "\n",
    "1. [TensorFlow's checkpointing mechanisms](https://www.tensorflow.org/programmers_guide/variables#saving_and_restoring)\n",
    "\n",
    "2. [SavedModelBuilder](https://www.tensorflow.org/api_docs/python/tf/saved_model/builder/SavedModelBuilder) -- This is particularly useful when you want to [create a prediction service using your trained model](https://tensorflow.github.io/serving/).\n",
    "\n",
    "The beauty of using the Estimator is that it handles checkpointing by itself and completely under the hood, so our only relationship to it here was specifying the `MODEL_DIR` into which we wanted our checkpoints (and metagraph information) to be stored. Checkpoints are automatically stored and loaded when you call any of the `Estimator` methods.\n",
    "\n",
    "Moreover, note that the [EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) that we return from our `model_fn` contains an `export_outputs` dictionary. This dictionary is used to provide information to the `SavedModelBuilder` when we want to export our graph using the Estimator's `export_saved_model()` method. We will see this in action after we cover training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQjw_10ou5D8"
   },
   "source": [
    "#### Training the estimator\n",
    "\n",
    "Let us begin by specifying our classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9LSWQ7d6u5D9"
   },
   "outputs": [],
   "source": [
    "distributions = [lambda n: sample_from_normal_distribution(n, MU, SIGMA),\n",
    "                 lambda n: sample_from_exponential_distribution(n, RATE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTPEv3RAu5EA"
   },
   "source": [
    "To begin with, let us train our estimator on a data set of 1000 samples from each distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-uVKqFYKu5EB"
   },
   "outputs": [],
   "source": [
    "regressor.train(generate_input_fn(distributions, [100000, 100000]), steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czBSh_WZu5EG"
   },
   "source": [
    "#### Evaluating the trained estimator\n",
    "\n",
    "In order to perform evaluation for one batch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 163,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192,
     "status": "error",
     "timestamp": 1500587187641,
     "user": {
      "displayName": "Neeraj Kashyap",
      "photoUrl": "//lh4.googleusercontent.com/-tQnVgNJEhwU/AAAAAAAAAAI/AAAAAAAAAH0/apPVgI_Oc5s/s50-c-k-no/photo.jpg",
      "userId": "116654443916371036868"
     },
     "user_tz": 420
    },
    "id": "qEFATVgXu5EH",
    "outputId": "1bdb823f-aeb3-4816-e9b2-3f5fdde1ee50"
   },
   "outputs": [],
   "source": [
    "regressor.evaluate(generate_input_fn(distributions, [1000, 1000]), steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkDDpVjUu5EL"
   },
   "source": [
    "#### Making predictions with the trained estimator\n",
    "\n",
    "In order to perform prediction for a single batch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1500584449767,
     "user": {
      "displayName": "Neeraj Kashyap",
      "photoUrl": "//lh4.googleusercontent.com/-tQnVgNJEhwU/AAAAAAAAAAI/AAAAAAAAAH0/apPVgI_Oc5s/s50-c-k-no/photo.jpg",
      "userId": "116654443916371036868"
     },
     "user_tz": 420
    },
    "id": "V6O-TBWbu5EN",
    "outputId": "97b5eff0-ecbf-4845-cb6c-92676b0bc0c5"
   },
   "outputs": [],
   "source": [
    "list(regressor.predict(generate_input_fn(distributions, [1, 1], num_epochs=1, shuffle=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the model\n",
    "\n",
    "The Estimator exposes its [export_savedmodel](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel) method, which is a very convenient way to manage a TensorFlow model, especially for serving purposes (either on your own or using the [Cloud ML Engine Prediction service](https://cloud.google.com/ml-engine/docs/concepts/prediction-overview)).\n",
    "\n",
    "When you use this `export_savedmodel` method, you will specify the following things:\n",
    "\n",
    "1. `export_dir_base` -- This is the base directory into which all the exports should go. The exports will be stored under a subdirectory of `expirt_dir_base` labelled by the timestamp at which the export took place.\n",
    "\n",
    "2. `serving_input_receiver_fn` -- A function which creates a [ServingInputReceiver](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver) object. This object specifies the inputs expected by the prediction server and how the features to be passed to the Estimator should be extracted from them.\n",
    "\n",
    "3. `assets_extra` -- Use this only if you need to provide extra assets with your `SavedModel`. We won't demonstrate this here. Set to `None` by default.\n",
    "\n",
    "4. `as_text` -- If you set this to `True`, the `SavedModel` will be exported in text format rather than as a `.proto` file. Set to `False` by default.\n",
    "\n",
    "5. `checkpoint_path` -- Here, you can optionally specify exactly which checkpoint you want to have exported from your `MODEL_DIR`. If you leave this with its default value of `None`, the most recent checkpoint will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### serving_input_receiver_fn\n",
    "\n",
    "We are pretty far down the rabbit hole now, but let's just focus on the interfaces before us. This is supposed to be a function with no arguments, which produces a [ServingInputReceiver](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver). A `ServingInputReceiver` is instantiated with two arguments -- `features`, and `receiver_tensors`. The `features` represent the inputs to our Estimator when it is being served. The `receiver_tensors` represent inputs to the server.\n",
    "\n",
    "In our case, we will expect inputs to any server serving our model to be exactly the features that we feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    feature_tensor = tf.placeholder(tf.float64, [None, 1])\n",
    "    return tf.estimator.export.ServingInputReceiver({'inputs': feature_tensor}, {'inputs': feature_tensor})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling export_savedmodel\n",
    "\n",
    "Now that we have a valid `serving_input_receiver_fn`, let us export the model into a designated export directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_EXPORT_DIR = setup_directory('DIRECTORY PATH GOES HERE - IT SHOULD NOT POINT TO AN EXISTING DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.export_savedmodel(BASE_EXPORT_DIR, serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serving a model\n",
    "\n",
    "(A brief aside on serving TensorFlow models.)\n",
    "\n",
    "Your best option if you want to serve a TensorFlow model that you have trained is to use [TensorFlow Serving](https://tensorflow.github.io/serving/). This is basically an application which serves your TensorFlow models through a gRPC API. It makes use of the `SavedModel`s exported by the Estimator's `export_savedmodel` method to do so.\n",
    "\n",
    "The [ML Engine](https://cloud.google.com/ml-engine/docs/concepts/technical-overview) on Google Cloud Platform offers a prediction service which manages TensorFlow Serving for you. All you have to do is provide it with versioned exports of your TensorFlow model, and it handles the serving portion for you. If you are interested in this, check out the [ML Engine Prediction Overview](https://cloud.google.com/ml-engine/docs/concepts/prediction-overview).\n",
    "\n",
    "For an example of how to use TensorFlow Serving in the real world, have a look at [this blog post](https://medium.com/zendesk-engineering/how-zendesk-serves-tensorflow-models-in-production-751ee22f0f4b) by Wai Chee Yau, who actually added multiple version serving functionality to the module!\n",
    "\n",
    "(Note: TensorFlow Serving does not integrate directly with Google Cloud Storage in the same way that TensorFlow does. You will have to download your exported SavedModels to the server on which you intend to run TensorFlow Serving.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "[tf.contrib.learn.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment) is a very useful aid in managing training and evaluation of a model. The class was originally designed for use with `tf.contrib.learn.Estimator`, but has been extended to work also with `tf.estimator.Estimator`. There are some nuances to making it work with `tf.estimator.Estimator`, however. We shall cover them in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunConfig\n",
    "\n",
    "Unfortunately, the integration with `tf.estimator.Estimator` is buggy. The source of the bug is essentially that the `Experiment` still wants the `Estimator` to be instantiated with [tf.contrib.learn.RunConfig](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig) whereas a `tf.estimator.Estimator` is typically instantiated with a [tf.estimator.RunConfig](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig).\n",
    "\n",
    "Thankfully, the attributes on a `tf.estimator.RunConfig` instance are a subset of those on a `tf.contrib.learn.RunConfig` instance, so we can simply use a `tf.contrib.learn.RunConfig` in place of what we had before. In the case of our `regressor`, we were using the default RunConfig. To make a regressor which works with `Experiment`, we simply add the `config` argument as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEW_MODEL_DIR = setup_directory('DIRECTORY PATH GOES HERE - IT SHOULD NOT POINT TO AN EXISTING DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_compatible_regressor = tf.estimator.Estimator(model_fn, model_dir=NEW_MODEL_DIR, params={'learning_rate': LEARNING_RATE},\n",
    "                                   config=tf.contrib.learn.RunConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export strategies\n",
    "\n",
    "Another notion to consider when dealing with `Experiment`s is that of [ExportStrategy](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/ExportStrategy). These dictate to the `Experiment` how to export various versions of the estimator for serving with even potentially different signatures as it performs (potentially continuous) training and evaluation.\n",
    "\n",
    "When working directly with an estimator, we would manually make calls to the `export_savedmodel` method with specific checkpoints and with specific `serving_input_receiver_fn`s in order to ahieve a similar effect.\n",
    "\n",
    "Now, there is a pretty useful utility for generating export strategies when you are using `tf.contrib.learn.Estimator` -- [tf.contrib.learn.make_export_strategy](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/make_export_strategy). However, this utility will not work for us because it requires an old-style `serving_input_fn`, which returns an [InputFnOps](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/InputFnOps) object as compared to a [ServingInputReceiver](https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver) object.\n",
    "\n",
    "Nevern mind, though, because it is quite easy for us to define our own export strategies very directly. The first thing to do is to export our export logic into an `export_fn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_fn(estimator, export_path, checkpoint_path=None):\n",
    "    return estimator.export_savedmodel(export_path,\n",
    "                                       serving_input_receiver_fn,\n",
    "                                       checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on `export_fn`s, you can refer directly to the [ExportStrategy](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/ExportStrategy) docs.\n",
    "\n",
    "The TL;DR version of it is that we can create an export strategy for use with our experiment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_strategy = tf.contrib.learn.ExportStrategy('default', export_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally!\n",
    "\n",
    "We can define our [experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = tf.contrib.learn.Experiment(experiment_compatible_regressor,\n",
    "                                         train_input_fn=generate_input_fn(distributions, [10000, 10000]),\n",
    "                                         eval_input_fn=generate_input_fn(distributions, [10, 10]),\n",
    "                                         train_steps=100,\n",
    "                                         export_strategies=export_strategy\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `train_steps=100`, we are saying that our experiment will only consist of training up to a `global_step` of 100. This is very significantly distinct from saying that we will train for 100 more `global_step`s. If we had already surpassed `global_step` 100, running train on this experiment would achieve no results!\n",
    "\n",
    "If we wanted to train forever, we would pass `train_steps=None`, which is actually the default.\n",
    "\n",
    "Now, we can train our estimator, evaluate it, and export the saved model for serving all with one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The saved models generated by `train_and_evaluate` are stored into the `exports` subdirectory of the regressor's `model_dir`. If you want to explicitly provide some other location, you can make the appropriate change to the `export_fn` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let us check on the performance on this new regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(experiment_compatible_regressor.predict(\n",
    "    generate_input_fn(distributions, [1, 1], num_epochs=1, shuffle=False)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Reading from the filesystem\n",
    "\n",
    "In the example above, we were generating all of our data (training, evaluation, and prediction) in memory. Although pedagogically convenient, this is practically useless to us in our everyday lives.\n",
    "\n",
    "In this section, we will rewire the whole example to read its training and evaluation data from disk. The beauty of it is that we will only need to make some very minor changes to our `input_fn` generator.\n",
    "\n",
    "First, we should produce our input files. We will write labelled data to 2-column CSV files in which the first column contains the label and the second column contains the number that was sampled from the distribution corresponding to the label.\n",
    "\n",
    "We will create `NUM_FILES` in `DATA_DIR`, each one having `SAMPLES_PER_FILE` points of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = setup_directory('DIRECTORY PATH GOES HERE - IT SHOULD NOT POINT TO AN EXISTING DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FILES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLES_PER_FILE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labelled_data_file(filename, distributions, labels):\n",
    "    labels_list = list(labels)\n",
    "    samples_list = [item[0] for item in map(lambda i: distributions[i](1), labels)]\n",
    "    rows = [','.join([str(entry) for entry in row]) for row in zip(labels_list, samples_list)]\n",
    "    content = '\\n'.join(rows)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = tuple(['{}/tf-estimator-data-{}.csv'.format(DATA_DIR, i) for i in range(NUM_FILES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(NUM_FILES):\n",
    "    # Generate SAMPLES_PER_FILE labels at random with each label having probability 0.5\n",
    "    labels = (np.random.uniform(size=SAMPLES_PER_FILE) > 0.5).astype(int)\n",
    "    create_labelled_data_file(filenames[i], distributions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the files have been created, we can define an `input_fn` generator which produces an Estimator input function that reads data from those files.\n",
    "\n",
    "This construction basically follows the [Reading Data](https://www.tensorflow.org/programmers_guide/reading_data) section of the [tensorflow.org Programmer's Guide](https://www.tensorflow.org/programmers_guide/). However, the comments in the code block below may be of some value to you if you could not go smoothly through the steps in the *Reading Data* guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_file_input_fn(filenames, num_epochs=None, shuffle=True):\n",
    "    def input_fn():\n",
    "        filename_queue = tf.train.string_input_producer(filenames,\n",
    "                                                        num_epochs,\n",
    "                                                        shuffle,\n",
    "                                                        capacity=len(filenames))\n",
    "        \n",
    "        file_reader = tf.TextLineReader()\n",
    "        _, csv = file_reader.read(filename_queue)\n",
    "        \n",
    "        # The record_defaults specify not only the defaults, but they also tell the decoder what\n",
    "        # schema to expect in the rows.\n",
    "        #\n",
    "        # The valid dtypes for record_defaults are tf.int32, tf.int64, tf.string, tf.float32.\n",
    "        # Since tf.float64 is not allowed and since our model_fn expects the inputs and the labels\n",
    "        # to have dtype=tf.float64, we will have to recast later.\n",
    "        record_defaults = [tf.constant([0], dtype=tf.float32),\n",
    "                           tf.constant([0], dtype=tf.float32)]\n",
    "        labels, raw_features = tf.decode_csv(csv, record_defaults=record_defaults)\n",
    "        \n",
    "        # Not only do we have to recast our features and labels to tf.float64, but we also\n",
    "        # have to explicitly specify the shape of each column. If we did not do this, TensorFlow\n",
    "        # would error out at graph construction time because it would not be able to validate\n",
    "        # the shapes of the tensors used in its operations.\n",
    "        #\n",
    "        # When designing your own estimator for release, it would be advisable to put this kind\n",
    "        # of code in your model_fn, making it easier for your users to provide custom input_fns.\n",
    "        processed_labels = tf.reshape(tf.cast(labels, tf.float64), [-1, 1])\n",
    "        processed_features = tf.reshape(tf.cast(raw_features, tf.float64), [-1, 1])\n",
    "        \n",
    "        return {'inputs': processed_features}, processed_labels\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use this to continue training our `experiment_compatible_regressor`. All we have to do is make a new `Experiment`!\n",
    "\n",
    "(This really highlights the utility of `tf.contrib.learn.Experiment`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_from_file_experiment = tf.contrib.learn.Experiment(experiment_compatible_regressor,\n",
    "                                                         train_input_fn=generate_file_input_fn(filenames),\n",
    "                                                         eval_input_fn=generate_input_fn(distributions, [10, 10]),\n",
    "                                                         train_steps=10000,\n",
    "                                                         export_strategies=export_strategy\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_file_experiment.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(experiment_compatible_regressor.predict(\n",
    "    generate_input_fn(distributions, [1, 1], num_epochs=1, shuffle=False)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder\n",
    "\n",
    "Don't forget to delete the directories you created through the course of this tutorial!\n",
    "\n",
    "I do not provide a way of deleting them here because too many things could go wrong deleting directories from a Jupyter notebook.\n",
    "\n",
    "These are the directories you should delete when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('REMOVE THESE DIRECTORIES:\\n1. {}\\n2. {}\\n3. {}\\n4. {}'.format(MODEL_DIR, NEW_MODEL_DIR, BASE_EXPORT_DIR, DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google, Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
